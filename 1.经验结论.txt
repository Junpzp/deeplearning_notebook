1.当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值。
2.He初始值
node_num = 100 # 前一层的节点数
w = np.random.randn(node_num, node_num) *np.sqrt(2/node_num)
3.Xavier初始值
node_num = 100 # 前一层的节点数
w = np.random.randn(node_num, node_num) *np.sqrt(1/node_num)
